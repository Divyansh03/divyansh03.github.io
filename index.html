<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Divyansh Jhunjhunwala</title>
  
  <meta name="author" content="Divyansh Jhunjhunwala">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text>üåê</svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:130%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Divyansh Jhunjhunwala</name>
              </p>

              <heading>About Me</heading>

		    
              <p> Hi! I am Divyansh, a third year PhD candidate in the Electrical and Computer Engineering department at Carnegie Mellon University, advised by <a href="https://www.andrew.cmu.edu/user/gaurij/">Dr. Gauri Joshi</a>. 
                My research interests lie broadly in distributed optimization and machine learning, in particular federated learning.
              </p>
              <p>
                In summer 22, I interned at IBM Research working with <a href="https://shiqiang.wang/">Dr. Shiqiang Wang</a> on some interesting problems in federated learning.
              </p>

              <p>

                Prior to CMU, I completed my Bachelors in Technology (B.Tech) in <a href="http://www.ecdept.iitkgp.ac.in/">Electronics and Electrical Communication Engineering</a> from <a href = "http://www.iitkgp.ac.in/">IIT Kharagpur</a>, where I
                received the Institute Silver Medal for graduating with the highest C.G.P.A in my department.
              </p>
              
              <p style="text-align:center">
                <a href="mailto:djhunjhu@andrew.cmu.edu">Email</a> &nbsp/&nbsp
                <a href="data/Divyansh_latest_CV_links.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=0E54wbUAAAAJ&hl=en&oi=ao">Google Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:70%;max-width:70%">
              <a href="data/divyansh_website_photo-modified.png"><img style="width:70%;max-width:70%" alt="profile photo" src="data/divyansh_website_photo-modified.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:130%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Recent News</heading>
              <p>
                <strong> Oct 22: </strong> Our work on <a href = "https://arxiv.org/abs/2205.14840"> incentivizing clients for federated learning </a> was accepted as an oral presentation at the <a href = "https://federated-learning.org/fl-neurips-2022/ "> FL-Neurips 22 workshop.</a> (12% acceptance rate).
              </p>
              <p>
                <strong> Aug 22: </strong> Completed my in-person internship at <a href="https://research.ibm.com/labs/watson/"> IBM T.J. Watson Research Center, New York</a>.
              </p>
              <p>
                <strong> April 22: </strong> Our team was selected as a finalist for the <a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2022-north-america"> Qualcomm Innovation Fellowship</a> for the research proposal 
		      "Incentivized Federated Learning for Data-Heterogeneous and Resource-Constrained Clients".
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:130%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:130%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/inc_fl.jpg" alt="inc fl image" width="230" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                <a href = "https://arxiv.org/abs/2205.14840"><papertitle>To Federate or Not To Federate: Incentivizing Client Participation in Federated Learning</papertitle> </a>
                  <br>
                  Yae Jee Cho,  <strong> Divyansh Jhunjhunwala </strong>, Tian Li, Virginia Smith, Gauri Joshi
                  <br>
                  <em>In submission</em>
                  <br>
                  <p>Propose IncFL algorithm to explicitly maximize the fraction of clients that are incentivized to use the global model in federated learning.</p>
                </td>
              </tr>

            
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/fedvarp.jpg" alt="fedvarp image" width="230" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                <a href = "https://arxiv.org/abs/2207.14130"><papertitle>FedVARP: Tackling the Variance Due to Partial Client Participation in Federated Learning</papertitle></a>
                  <br>
                  <strong> Divyansh Jhunjhunwala </strong>, Pranay Sharma, Aushim Nagarkatti, Gauri Joshi
                  <br>
                  <em> Uncertainty in Artificial Intelligence, 2022</em>
                  <br>
                  <p>Propose FedVARP algorithm to deal with variance caused by only a few clients participating in every round of federated training.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/spatial_temporal.jpg" alt="spatial image" width="230" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2110.07751"><papertitle> Leveraging Spatial and Temporal Correlations in Sparsified Mean Estimation </papertitle></a>
                  <br>
                  <strong> Divyansh Jhunjhunwala </strong>, Ankur Mallick, Advait Gadhikar, Swanand Kadhe, Gauri Joshi
                  <br>
                  <em> Advances in Neural Information Processing (NeurIPS), 2021</em>
                  <br>
                  <p> Introduce notions of spatial and temporal correlations and show how they can be used to efficiently compute the mean of a set of vectors in a communication-limited setting. </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/adaquant_fl.jpg" alt="adaquant fl image" width="230" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                <a href = "https://arxiv.org/abs/2102.04487"><papertitle> Adaptive Quantization of model updates for communication-efficient federated learning  </papertitle></a>
                  <br>
                  <strong> Divyansh Jhunjhunwala </strong>, Advait Gadhikar, Gauri Joshi, Yonina C. Eldar
                  <br>
                  <em> International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021</em>
                  <br>
                  <p> Propose an adaptive quantization strategy that aims to achieve communication efficiency as well as a low error floor by changing the number of quantization levels during training in federated learning.</p>
                </td>
              </tr>

        </tbody></table>





        <table style="width:120%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                    Source code credit to <a href ="https://github.com/jonbarron/website "> Dr. Jon Barron. </a>
                </p>
              </td>
            </tr>
        </tbody></table>

            
      </td>
    </tr>
  </table>
</body>

</html>
